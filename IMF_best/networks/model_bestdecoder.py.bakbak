from pickle import FALSE
from regex import P
import torch
import torch.nn as nn
import torch.nn.functional as F
import torchvision.models as models
from torch.utils.checkpoint import checkpoint
import torch.nn.utils.spectral_norm as spectral_norm
from utils.modules import FeatResBlock,UpConvResBlock,DownConvResBlock,ResBlock, SPADEDecoder
from utils.lia_resblocks import StyledConv,EqualConv2d,EqualLinear
from utils.vit import ImplicitMotionAlignment
import math
from utils.modules import ConvResBlock
from utils.vit import TransformerBlock, CrossAttentionModule
class DenseFeatureEncoder(nn.Module):
    def __init__(self, in_channels=3, output_channels=[128, 256, 512, 512], initial_channels=64, dm=32):
        super().__init__()
        
        # Initial convolution layer with BatchNorm and ReLU
        self.initial_conv = nn.Sequential(
            nn.Conv2d(in_channels, initial_channels, kernel_size=7, stride=1, padding=3),
            nn.BatchNorm2d(initial_channels),  # Stable eps for better numerical stability
            nn.ReLU(inplace=True)
        )
        
        # List of downsampling blocks (DownConvResBlocks)
        self.down_blocks = nn.ModuleList()
        current_channels = initial_channels
        
        # Initial block that doesn't change the number of channels
        self.down_blocks.append(DownConvResBlock(current_channels, current_channels))
        
        # Add down blocks for each specified output channel
        for out_channels in output_channels:
            self.down_blocks.append(DownConvResBlock(current_channels, out_channels))
            current_channels = out_channels

    def forward(self, x):
        features = []
        x = self.initial_conv(x)
        # Apply downsampling blocks and collect features from the second block onwards
        for i, block in enumerate(self.down_blocks):
            x = block(x)
            if i >= 1:  # Collect features from the second block onwards
                features.append(x)

        return features


class LatentTokenEncoder(nn.Module):
    def __init__(self, initial_channels=64, output_channels=[64, 128, 256, 512, 512, 512], dm=32):
        super(LatentTokenEncoder, self).__init__()

        # Initial convolution followed by LeakyReLU activation
        self.conv1 = nn.Conv2d(3, initial_channels, kernel_size=3, stride=1, padding=1)
        self.activation = nn.LeakyReLU(0.2)

        # Dynamically create ResBlocks
        self.res_blocks = nn.ModuleList()
        in_channels = initial_channels
        for out_channels in output_channels:
            self.res_blocks.append(ResBlock(in_channels, out_channels))
            in_channels = out_channels

        # Equal convolution and linear layers
        self.equalconv = EqualConv2d(output_channels[-1], output_channels[-1], kernel_size=3, stride=1, padding=1)
        self.linear_layers = nn.ModuleList([EqualLinear(output_channels[-1], output_channels[-1]) for _ in range(4)])
        self.final_linear = EqualLinear(output_channels[-1], dm)

    def forward(self, x):
        # Initial convolution and activation
        x = self.activation(self.conv1(x))
        
        # Apply ResBlocks
        for res_block in self.res_blocks:
            x = res_block(x)
        
        # Apply equalconv
        x = self.equalconv(x)
        
        # Global average pooling
        x = x.view(x.size(0), x.size(1), -1).mean(dim=2)
        
        # Apply linear layers
        for linear_layer in self.linear_layers:
            x = self.activation(linear_layer(x))
            
        
        # Final linear layer
        x = self.final_linear(x)
        
        return x


class LatentTokenDecoder(nn.Module):
    def __init__(self, latent_dim=32, const_dim=32):
        super().__init__()
        # Constant input for the decoder
        self.const = nn.Parameter(torch.randn(1, const_dim, 4, 4))
        
        # StyleConv layers
        self.style_conv_layers = nn.ModuleList([
            StyledConv(const_dim, 512, 3, latent_dim),
            StyledConv(512, 512, 3, latent_dim, upsample=True),
            StyledConv(512, 512, 3, latent_dim),
            StyledConv(512, 512, 3, latent_dim),
            StyledConv(512, 512, 3, latent_dim, upsample=True),
            StyledConv(512, 512, 3, latent_dim),
            StyledConv(512, 512, 3, latent_dim),
            StyledConv(512, 256, 3, latent_dim, upsample=True),
            StyledConv(256, 256, 3, latent_dim),
            StyledConv(256, 256, 3, latent_dim),
            StyledConv(256, 128, 3, latent_dim, upsample=True),
            StyledConv(128, 128, 3, latent_dim),
            StyledConv(128, 128, 3, latent_dim)  
        ])

    def forward(self, t):
        # Repeat constant input for batch size
        x = self.const.repeat(t.shape[0], 1, 1, 1)
        
        # Store feature maps
        m1, m2, m3, m4 = None, None, None, None
        # Apply style convolution layers
        for i, layer in enumerate(self.style_conv_layers):
            x = layer(x, t)
            
            if i == 3:
                m1 = x
            elif i == 6:
                m2 = x
            elif i == 9:
                m3 = x
            elif i == 12:
                m4 = x
        
        # Return the feature maps in reverse order
        return m4, m3, m2, m1

    
class FrameDecoder(nn.Module):
    def __init__(self, feature_dims, spatial_dims, depth):
        """
        ä¸€ä¸ªå¥å£®çš„ã€æ”¯æŒåŒç‰¹å¾æµèåˆçš„è§£ç å™¨ã€‚
        
        Args:
            feature_dims (list): ç¼–ç å™¨è¾“å‡ºçš„å„å°ºåº¦ç‰¹å¾é€šé“æ•°ï¼Œä»æµ…åˆ°æ·±ã€‚
                                 ä¾‹å¦‚: [64, 128, 256, 512]
            spatial_dims (list): ç¼–ç å™¨è¾“å‡ºçš„å„å°ºåº¦ç‰¹å¾ç©ºé—´å°ºå¯¸ï¼Œä»æµ…åˆ°æ·±ã€‚
                                 ä¾‹å¦‚: [128, 64, 32, 16]
            depth (int): æ¯ä¸ªå°ºåº¦ä¸ŠTransformerBlockçš„é‡å¤æ¬¡æ•°ã€‚
        """
        super().__init__()
        
        # åè½¬åˆ—è¡¨ï¼Œæ–¹ä¾¿ç”±æ·±åˆ°æµ…è¿›è¡Œç´¢å¼• (0=æœ€æ·±å±‚)
        feature_dims = feature_dims[::-1]
        spatial_dims = spatial_dims[::-1]
        
        # ------------------ æ¨¡å—å®šä¹‰åŒº ------------------

        # 1. ç”¨äºå¤„ç†æœ€æ·±å±‚èåˆçš„åˆå§‹æ¨¡å—
        #    è¾“å…¥: srcæœ€æ·±å±‚ + alignæœ€æ·±å±‚
        self.conv_in = ConvResBlock(feature_dims[0], feature_dims[0])

        # 2. ä¸Šé‡‡æ ·æ¨¡å—åˆ—è¡¨
        #    å°†ç‰¹å¾ä»æ·±å±‚(i)ä¸Šé‡‡æ ·è‡³ä¸‹ä¸€å±‚(i+1)
        self.upconv_blocks = nn.ModuleList([
            UpConvResBlock(feature_dims[i], feature_dims[i+1]) for i in range(len(feature_dims) - 1)
        ])

        # 3. å·ç§¯æ®‹å·®æ¨¡å—åˆ—è¡¨
        #    ç”¨äºå¤„ç†èåˆåçš„ä¸‰è‚¡ç‰¹å¾
        #    è¾“å…¥: ä¸Šé‡‡æ ·ç‰¹å¾ + srcç‰¹å¾ + alignç‰¹å¾
        self.resblocks = nn.ModuleList([
            ConvResBlock(feature_dims[i+1] * 2, feature_dims[i+1]) for i in range(len(feature_dims) - 1)
        ])
        
        # 4. Transformeræ¨¡å—åˆ—è¡¨
        #    ç”¨äºåœ¨æ¯ä¸ªå°ºåº¦ä¸Šè¿›è¡Œç‰¹å¾å¢å¼º
        self.transformer_blocks = nn.ModuleList([
            # æœ€æ·±å±‚ (å¾ªç¯å¤–å¤„ç†)
            nn.Sequential(*[TransformerBlock(dim=feature_dims[0], heads=8, dim_spatial=spatial_dims[0]**2, mlp_dim=1024) for _ in range(depth)]),
            # åç»­å±‚ (å¾ªç¯å†…å¤„ç†)
            *[nn.Sequential(*[TransformerBlock(dim=feature_dims[i+1], heads=8, dim_spatial=spatial_dims[i+1]**2, mlp_dim=1024) for _ in range(depth)]) for i in range(len(feature_dims) - 1)]
        ])

        # 5. æœ€ç»ˆè¾“å‡ºå±‚
        #self.final_conv = nn.Sequential(
        #    UpConvResBlock(feature_dims[-1], 64),
        #    UpConvResBlock(64, 32),
        #    nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),
        #    nn.Conv2d(32, 3, kernel_size=3, padding=1),
        #    nn.Sigmoid(),
        #)
        self.final_conv = SPADEDecoder(upscale=2, max_features=128)

    def forward(self, features_align):
        """
        Args:
            features_src (list): æºç¼–ç å™¨çš„ç‰¹å¾åˆ—è¡¨ï¼Œä»æµ…åˆ°æ·±ã€‚
            features_align (list): å¯¹é½ç¼–ç å™¨çš„ç‰¹å¾åˆ—è¡¨ï¼Œä»æµ…åˆ°æ·±ã€‚
        """
        
        # ------------------ å‰å‘ä¼ æ’­åŒº ------------------

        # 1. å¤„ç†æœ€æ·±å±‚ (è§£ç èµ·ç‚¹)
        #    èåˆsrcå’Œalignçš„æœ€æ·±å±‚ç‰¹å¾
        x = features_align[-1]

        x = self.conv_in(x)
        x = self.transformer_blocks[0](x) # å¯¹æœ€æ·±å±‚çš„èåˆç»“æœåº”ç”¨Transformer

        # 2. å¾ªç¯å¤„ç†åç»­å±‚ï¼Œç”±æ·±åˆ°æµ…
        #    å¾ªç¯æ¬¡æ•° = ä¸Šé‡‡æ ·æ¨¡å—çš„æ•°é‡
        for i in range(len(self.upconv_blocks)):
            
            # (a) ä¸Šé‡‡æ ·
            x = self.upconv_blocks[i](x)
            
            # (b) èåˆä¸‰è‚¡ç‰¹å¾ï¼šä¸Šé‡‡æ ·ç‰¹å¾ã€srcç‰¹å¾ã€alignç‰¹å¾
            #     -i-2 ä¼šä»åˆ—è¡¨æœ«å°¾å¼€å§‹ä¾æ¬¡å‘å‰å–å€¼ï¼Œä¾‹å¦‚-2, -3, -4...
            align_skip = features_align[-(i + 2)]
            x = torch.cat([x, align_skip], dim=1)
            
            # (c) é€šè¿‡ResBlockè¿›è¡Œæ·±åº¦èåˆ
            x = self.resblocks[i](x)
            
            # (d) é€šè¿‡Transformerè¿›è¡Œå…¨å±€ç‰¹å¾å¢å¼º
            #     transformer_blocks[0]å·²ç”¨äºæœ€æ·±å±‚ï¼Œæ‰€ä»¥è¿™é‡Œä»[1]å¼€å§‹
            x = self.transformer_blocks[i + 1](x)
            
        # 3. é€šè¿‡æœ€ç»ˆè¾“å‡ºå±‚ç”Ÿæˆå›¾åƒ
        out = self.final_conv(x)
        return out

class IMFModel(nn.Module):
    '''
    IMFModel consists of the following components:
    - DenseFeatureEncoder (EF): Encodes the reference frame into multi-scale features.
    - LatentTokenEncoder (ET): Encodes both current and reference frames into latent tokens.
    - LatentTokenDecoder (IMFD): Decodes latent tokens into motion features.
    - ImplicitMotionAlignment (IMFA): Aligns reference features to the current frame using motion features.
    '''

    def __init__(self, args):
        super().__init__()
        self.latent_token_encoder = LatentTokenEncoder(initial_channels=64, output_channels=[128, 256, 512, 512, 512])
        self.latent_token_decoder = LatentTokenDecoder()

        self.feature_dims = [128, 256, 512, 512]
        self.spatial_dims = [64, 32, 16, 8]
        self.motion_dims = [128, 256, 512, 512]

        self.dense_feature_encoder = DenseFeatureEncoder(output_channels=self.feature_dims)

        # Initialize ImplicitMotionAlignment modules
        self.implicit_motion_alignment = nn.ModuleList(
            [CrossAttentionModule(dim_spatial=s * s, dim_qk=m, dim_v=f) for s, m, f in zip(self.spatial_dims, self.motion_dims, self.feature_dims)]
        )

        self.frame_decoder = FrameDecoder(self.feature_dims, self.spatial_dims, args.depth)

    def encode_dense_feature(self, x_reference):
        f_r = self.dense_feature_encoder(x_reference)
        return f_r

    def encode_latent_token(self, x_reference):
        t_c = self.latent_token_encoder(x_reference)
        return t_c

    def tokens(self, x_current, x_reference):
        f_r = self.dense_feature_encoder(x_reference)
        t_r, t_c = self.latent_token_encoder(x_reference), self.latent_token_encoder(x_current)
        return f_r, t_r, t_c

    def decode_latent_tokens(self, f_r, t_r, t_c):
        m_c, m_r = self.latent_token_decoder(t_c), self.latent_token_decoder(t_r)
        aligned_features = [
            align_layer(m_c_i, m_r_i, f_r_i)  # ä¼ é€’ mask_i
            for m_c_i, m_r_i, f_r_i,align_layer in zip(m_c, m_r, f_r,  self.implicit_motion_alignment)
        ]
        return self.frame_decoder(aligned_features)

    def ima(self, m_c, m_r, f_r):
        aligned_features = [
            align_layer(m_c_i, m_r_i, f_r_i)  # ä¼ é€’ mask_i
            for m_c_i, m_r_i, f_r_i,align_layer in zip(m_c, m_r, f_r,  self.implicit_motion_alignment)
        ]
        return self.frame_decoder(aligned_features)

    def forward(self, x_current, x_reference):
        f_r, t_r, t_c = self.tokens(x_current, x_reference)

        m_c, m_r = self.latent_token_decoder(t_c), self.latent_token_decoder(t_r)   

        aligned_features = [
        align_layer(m_c_i, m_r_i, f_r_i)  # ä¼ é€’ mask_i
        for m_c_i, m_r_i, f_r_i,align_layer in zip(m_c, m_r, f_r, self.implicit_motion_alignment)
        ]
        
        return self.frame_decoder(aligned_features)
    
if __name__ == "__main__":
    import torch
    import torch.nn as nn
    import time
    import argparse
    args = argparse.Namespace()
    args.depth = 4
    # å‡è®¾ä½ å·²ç»å®šä¹‰äº† IMFModel åŠå…¶ä¾èµ–çš„æ¨¡å—
    model = IMFModel(args).cuda()
    model.eval()

    # æ‰“å°å„æ¨¡å—å‚æ•°é‡
    def count_parameters(module):
        return sum(p.numel() for p in module.parameters() if p.requires_grad)

    print("ğŸ” æ¨¡å—å‚æ•°ç»Ÿè®¡ï¼š")
    for name, submodule in model.named_children():
        print(f"{name:30s}: {count_parameters(submodule):,} å‚æ•°")

    print(f"{'å…¨éƒ¨æ¨¡å‹':30s}: {count_parameters(model):,} å‚æ•°")

    # éšæœºè¾“å…¥æµ‹è¯•
    x_current = torch.randn(1000, 3, 256, 256).cuda()
    x_reference = torch.randn(1, 3, 256, 256).cuda()
    mask_list = [ torch.randn(1, 256, 256).cuda()]

    # å‰å‘ä¼ æ’­è€—æ—¶
    print("\nâ±ï¸ æ­£åœ¨è¿è¡Œ 1000 æ¬¡å‰å‘ä¼ æ’­...")
    start_time = time.time()

    with torch.no_grad():
        f_r = model.encode_dense_feature(x_reference)
        t_r = model.encode_latent_token(x_reference)
        for i in range(1000):
            x = x_current[i].unsqueeze(dim=0)
            t_c = model.encode_latent_token(x)
            _ = model.decode_latent_tokens(f_r, t_r, t_c)

    end_time = time.time()
    print(f"âœ… å®Œæˆï¼æ€»è€—æ—¶: {end_time - start_time:.2f} ç§’ï¼Œå¹³å‡æ¯æ¬¡: {(end_time - start_time)/1000:.4f} ç§’")
