# æ–‡ä»¶å: imf_model.py
# (ä¸»è¦ä¿®æ”¹ç‚¹: FrameDecoder)

import torch
import torch.nn as nn
import argparse

# --- æ ¸å¿ƒä¾èµ–å¯¼å…¥ ---
from utils.modules import (
    DownConvResBlock, ResBlock, UpConvResBlock, ConvResBlock, 
)
# å¯¼å…¥æ‰€æœ‰æ³¨æ„åŠ›æ¨¡å—ï¼ŒåŒ…æ‹¬æ–°å¢çš„ SwinTransformerBlock
from utils.attention_modules import AttentionLayerFactory, CrossAttentionLayerFactory
from utils.lia_resblocks import StyledConv,EqualConv2d,EqualLinear
# ... LatentTokenEncoder, DenseFeatureEncoder, LatentTokenDecoder çš„ä»£ç ä¿æŒä¸å˜ ...
class DenseFeatureEncoder(nn.Module):
    # ... (ä»£ç ä¸å˜) ...
    def __init__(self, in_channels=3, output_channels=[64, 128, 256, 512, 512, 512], initial_channels=32):
        super().__init__()
        self.initial_conv = nn.Sequential(
            nn.Conv2d(in_channels, initial_channels, kernel_size=7, stride=1, padding=3),
            nn.BatchNorm2d(initial_channels),
            nn.ReLU(inplace=True)
        )
        self.down_blocks = nn.ModuleList()
        current_channels = initial_channels
        for out_channels in output_channels:
            if out_channels==32:continue
            self.down_blocks.append(DownConvResBlock(current_channels, out_channels))
            current_channels = out_channels
    def forward(self, x):
        features = []
        x = self.initial_conv(x)
        features.append(x)
        for block in self.down_blocks:
            x = block(x)
            features.append(x)
        return features

class LatentTokenEncoder(nn.Module):
    def __init__(self, initial_channels=64, output_channels=[64, 128, 256, 512, 512, 512], dm=32):
        super(LatentTokenEncoder, self).__init__()

        # Initial convolution followed by LeakyReLU activation
        self.conv1 = nn.Conv2d(3, initial_channels, kernel_size=3, stride=1, padding=1)
        self.activation = nn.LeakyReLU(0.2)

        # Dynamically create ResBlocks
        self.res_blocks = nn.ModuleList()
        in_channels = initial_channels
        for out_channels in output_channels:
            self.res_blocks.append(ResBlock(in_channels, out_channels))
            in_channels = out_channels

        # Equal convolution and linear layers
        self.equalconv = EqualConv2d(output_channels[-1], output_channels[-1], kernel_size=3, stride=1, padding=1)
        self.linear_layers = nn.ModuleList([EqualLinear(output_channels[-1], output_channels[-1]) for _ in range(4)])
        self.final_linear = EqualLinear(output_channels[-1], dm)

    def forward(self, x):
        # Initial convolution and activation
        x = self.activation(self.conv1(x))
        
        # Apply ResBlocks
        for res_block in self.res_blocks:
            x = res_block(x)
        
        # Apply equalconv
        x = self.equalconv(x)
        
        # Global average pooling
        x = x.view(x.size(0), x.size(1), -1).mean(dim=2)
        
        # Apply linear layers
        for linear_layer in self.linear_layers:
            x = self.activation(linear_layer(x))
            
        
        # Final linear layer
        x = self.final_linear(x)
        
        return x

class LatentTokenDecoder(nn.Module):
    def __init__(self, latent_dim=32, const_dim=32):
        super().__init__()
        # Constant input for the decoder
        self.const = nn.Parameter(torch.randn(1, const_dim, 4, 4))
        
        # StyleConv layers
        self.style_conv_layers = nn.ModuleList([
            StyledConv(const_dim, 512, 3, latent_dim),
            StyledConv(512, 512, 3, latent_dim, upsample=True),
            StyledConv(512, 512, 3, latent_dim),
            StyledConv(512, 512, 3, latent_dim),
            StyledConv(512, 512, 3, latent_dim, upsample=True),
            StyledConv(512, 512, 3, latent_dim),
            StyledConv(512, 512, 3, latent_dim),
            StyledConv(512, 256, 3, latent_dim, upsample=True),
            StyledConv(256, 256, 3, latent_dim),
            StyledConv(256, 256, 3, latent_dim),
            StyledConv(256, 128, 3, latent_dim, upsample=True),
            StyledConv(128, 128, 3, latent_dim),
            StyledConv(128, 128, 3, latent_dim)  
        ])

    def forward(self, t):
        # Repeat constant input for batch size
        x = self.const.repeat(t.shape[0], 1, 1, 1)
        #import pdb;pdb.set_trace()
        # Store feature maps
        m1, m2, m3, m4 = None, None, None, None
        # Apply style convolution layers
        for i, layer in enumerate(self.style_conv_layers):
            x = layer(x, t)
            
            if i == 3:
                m1 = x
            elif i == 6:
                m2 = x
            elif i == 9:
                m3 = x
            elif i == 12:
                m4 = x
        
        # Return the feature maps in reverse order
        return m4,m4, m4, m3, m2, m1,

# ============================================================================
# ä¸»æ¨¡å‹ (æœ€ç»ˆç®€åŒ–ç‰ˆ)
# ============================================================================
class FrameDecoder(nn.Module):
    def __init__(self, args, feature_dims, spatial_dims):
        super().__init__()
        self.args = args
        
        feature_dims_rev = feature_dims[::-1]
        spatial_dims_rev = spatial_dims[::-1]

        self.upconv_blocks = nn.ModuleList([
            UpConvResBlock(feature_dims_rev[i], feature_dims_rev[i+1]) for i in range(len(feature_dims_rev) - 1)
        ])
        self.resblocks = nn.ModuleList([
            ConvResBlock(feature_dims_rev[i+1]*2, feature_dims_rev[i+1]) for i in range(len(feature_dims_rev) - 1)
        ])
        
        self.transformer_blocks = nn.ModuleList()
        print("ğŸ”§ æ­£åœ¨é€šè¿‡å·¥å‚æ„å»ºè§£ç å™¨ä¸­çš„ç»Ÿä¸€è‡ªæ³¨æ„åŠ›å±‚:")
        for i in range(len(spatial_dims_rev) - 1):
            s_dim = spatial_dims_rev[i+1]
            f_dim = feature_dims_rev[i+1]
            self.transformer_blocks.append(
                AttentionLayerFactory(args=args, dim=f_dim, resolution=(s_dim, s_dim))
            )

        self.final_conv = nn.Sequential(
            nn.LeakyReLU(0.2, inplace=True),
            nn.Conv2d(feature_dims_rev[-1], 3*4, kernel_size=3, padding=1),
            nn.PixelShuffle(upscale_factor=2),
            nn.Sigmoid()
        )

    def forward(self, features_align):
        x = features_align[-1]
        #import pdb;pdb.set_trace()
        for i in range(len(self.upconv_blocks)):
            x = self.upconv_blocks[i](x)
            x = torch.cat([x, features_align[-(i + 2)]], dim=1)
            x = self.resblocks[i](x)
            x = self.transformer_blocks[i](x)
        return self.final_conv(x)


class IMFModel(nn.Module):
    def __init__(self, args):
        super().__init__()
        self.args = args
        self.feature_dims = [32, 64, 128, 256, 512, 512]
        self.motion_dims = self.feature_dims
        self.spatial_dims = [256, 128, 64, 32, 16, 8]

        self.dense_feature_encoder = DenseFeatureEncoder(output_channels=self.feature_dims)
        self.latent_token_encoder = LatentTokenEncoder(initial_channels=64, output_channels=[128, 256, 512, 512, 512])
        self.latent_token_decoder = LatentTokenDecoder()
        
        self.frame_decoder = FrameDecoder(args, self.feature_dims, self.spatial_dims)

        self.implicit_motion_alignment = nn.ModuleList()
        print("ğŸ”§ æ­£åœ¨é€šè¿‡å·¥å‚æ„å»ºå¯¹é½é˜¶æ®µçš„ç»Ÿä¸€äº¤å‰æ³¨æ„åŠ›å±‚:")
        for dim, s_dim in zip(self.feature_dims, self.spatial_dims):
            self.implicit_motion_alignment.append(
                CrossAttentionLayerFactory(args=args, dim=dim, resolution=(s_dim, s_dim))
            )

    def forward(self, x_current, x_reference):
        f_r = self.dense_feature_encoder(x_reference)
        m_c = self.latent_token_decoder(self.latent_token_encoder(x_current))
        m_r = self.latent_token_decoder(self.latent_token_encoder(x_reference))
        
        num_levels = len(self.spatial_dims)
        aligned_features = [None] * num_levels
        attention_map = None # åˆå§‹åŒ– attention_map ä¸º None

        # --- æ ¸å¿ƒä¿®æ­£ï¼šä»æœ€ç²—ç³™çš„å±‚çº§å¼€å§‹ï¼Œåå‘å¾ªç¯ ---
        # reversed(range(num_levels)) ä¼šç”Ÿæˆ 5, 4, 3, 2, 1, 0
        for i in reversed(range(num_levels)):
            
            # ä½¿ç”¨ç»Ÿä¸€çš„ç´¢å¼• i æ¥è·å–æ‰€æœ‰å¯¹åº”çš„ç‰¹å¾å’Œæ¨¡å—
            query_feat = m_c[i]
            key_feat = m_r[i]
            value_feat = f_r[i]
            attention_block = self.implicit_motion_alignment[i]

            # æ£€æŸ¥å½“å‰å±‚çº§æ˜¯å¦æ˜¯ç²—ç²’åº¦å±‚ï¼ˆä½¿ç”¨æ ‡å‡†å…¨æ³¨æ„åŠ›ï¼‰
            # is_standard_attention åº”è¯¥æ˜¯åœ¨å·¥å‚ __init__ ä¸­è®¾ç½®çš„å±æ€§
            if attention_block.is_standard_attention:
                # è¿™æ˜¯ç²—ç²’åº¦å±‚ã€‚å¦‚æœ attention_map æ˜¯ Noneï¼Œè¯´æ˜è¿™æ˜¯ç¬¬ä¸€å±‚ï¼ˆæœ€ç²—ç³™çš„8x8å±‚ï¼‰ã€‚
                # å®ƒä¼šè®¡ç®—å¹¶è¿”å›æ–°çš„ attention_mapã€‚
                # å³ä¾¿ä¸æ˜¯ç¬¬ä¸€å±‚ç²—ç²’åº¦å±‚ï¼ˆå¦‚16x16ï¼‰ï¼Œå®ƒä¹Ÿä¼šè®¡ç®—è‡ªå·±çš„å…¨æ³¨æ„åŠ›å›¾ï¼Œå¹¶è¦†ç›–æ‰ä¹‹å‰çš„ã€‚
                aligned_feature, attention_map = attention_block(query_feat, key_feat, value_feat)
                aligned_features[i] = aligned_feature
            else:
                # è¿™æ˜¯ç²¾ç»†ç²’åº¦å±‚ï¼Œå¿…é¡»æ¥æ”¶æ¥è‡ªä¸Šä¸€å±‚ï¼ˆæ›´ç²—ç³™å±‚ï¼‰çš„ attention_map
                if attention_map is None:
                    raise RuntimeError("ç²¾ç»†ç²’åº¦å±‚æ²¡æœ‰æ”¶åˆ°æ¥è‡ªç²—ç²’åº¦å±‚çš„ attention map æŒ‡å¯¼ã€‚")
                
                # è°ƒç”¨å¼•å¯¼å¼ç¨€ç–æ³¨æ„åŠ›ï¼Œå®ƒåªéœ€è¦ä¸€ä¸ªè¿”å›å€¼
                aligned_feature = attention_block(query_feat, key_feat, value_feat, attn=attention_map)
                aligned_features[i] = aligned_feature
                
        
        # aligned_features åˆ—è¡¨ç°åœ¨å·²æŒ‰ fine -> coarse çš„é¡ºåºæ­£ç¡®å¡«å……
        output_frame = self.frame_decoder(aligned_features)
        return output_frame
import torch
import torch.nn as nn
import time
from collections import defaultdict
import numpy as np

# ============================================================================
# Part 1: æ–°å¢çš„æ¨¡å—è®¡æ—¶å·¥å…·ç±»
# ============================================================================
class ModuleTimer:
    """
    ä¸€ä¸ªä½¿ç”¨ PyTorch Hooks æ¥ä¸º nn.Module è®¡æ—¶çš„å·¥å…·ç±»ã€‚
    å®ƒä¸ä¿®æ”¹æ¨¡å‹æ¥å£ï¼Œå¹¶ä¸”èƒ½ç²¾ç¡®æµ‹é‡ CUDA ä¸Šçš„æ‰§è¡Œæ—¶é—´ã€‚
    """
    def __init__(self, modules_to_time, device):
        self.modules_to_time = modules_to_time
        self.device = device
        self.timings = defaultdict(list)
        self.start_events = {}

        self._register_hooks()

    def _start_timing(self, name):
        """è®°å½•å¼€å§‹æ—¶é—´ç‚¹ã€‚"""
        if self.device.type == 'cuda':
            # ç¡®ä¿ä¹‹å‰çš„CUDAæ“ä½œå®Œæˆ
            torch.cuda.synchronize()
            start_event = torch.cuda.Event(enable_timing=True)
            start_event.record()
            self.start_events[name] = start_event
        else:
            self.start_events[name] = time.time()

    def _stop_timing(self, name):
        """è®°å½•ç»“æŸæ—¶é—´ç‚¹å¹¶è®¡ç®—è€—æ—¶ã€‚"""
        if name not in self.start_events:
            return  # å¦‚æœæ²¡æœ‰å¼€å§‹äº‹ä»¶ï¼Œåˆ™è·³è¿‡

        start_event_or_time = self.start_events[name]

        if self.device.type == 'cuda':
            end_event = torch.cuda.Event(enable_timing=True)
            end_event.record()
            # ç¡®ä¿ç»“æŸäº‹ä»¶ä¹Ÿå®Œæˆ
            torch.cuda.synchronize()
            # elapsed_time è¿”å›æ¯«ç§’ (ms)
            duration_ms = start_event_or_time.elapsed_time(end_event)
            self.timings[name].append(duration_ms)
        else:
            end_time = time.time()
            duration_ms = (end_time - start_event_or_time) * 1000
            self.timings[name].append(duration_ms)

        # æ¸…ç†å·²ä½¿ç”¨çš„äº‹ä»¶
        del self.start_events[name]

    def _make_pre_hook(self, name):
        """åˆ›å»ºä¸€ä¸ªå‰å‘ä¼ æ’­å‰çš„é’©å­å‡½æ•°ã€‚"""
        def pre_hook(module, input):
            self._start_timing(name)
        return pre_hook

    def _make_forward_hook(self, name):
        """åˆ›å»ºä¸€ä¸ªå‰å‘ä¼ æ’­åçš„é’©å­å‡½æ•°ã€‚"""
        def forward_hook(module, input, output):
            self._stop_timing(name)
        return forward_hook

    def _register_hooks(self):
        """ä¸ºæ‰€æœ‰æŒ‡å®šçš„æ¨¡å—æ³¨å†Œé’©å­ã€‚"""
        for name, module in self.modules_to_time.items():
            if isinstance(module, nn.ModuleList) or isinstance(module, list):
                 # å¦‚æœæ˜¯åˆ—è¡¨ï¼Œä¸ºåˆ—è¡¨ä¸­çš„æ¯ä¸ªå­æ¨¡å—æ³¨å†Œé’©å­
                for i, sub_module in enumerate(module):
                    sub_name = f"{name}_{i}"
                    sub_module.register_forward_pre_hook(self._make_pre_hook(sub_name))
                    sub_module.register_forward_hook(self._make_forward_hook(sub_name))
            else:
                 module.register_forward_pre_hook(self._make_pre_hook(name))
                 module.register_forward_hook(self._make_forward_hook(name))

    def reset(self):
        """é‡ç½®è®¡æ—¶å™¨ï¼Œæ¸…é™¤æ‰€æœ‰è®°å½•çš„æ—¶é—´ã€‚"""
        self.timings.clear()

    def summary(self):
        """æ‰“å°è®¡æ—¶ç»“æœçš„æ‘˜è¦è¡¨æ ¼ã€‚"""
        if not self.timings:
            print("No timings recorded.")
            return

        print("\n" + "="*80)
        print("ğŸ“Š å¹³å‡è¿è¡Œé€Ÿåº¦åˆ†æ (ms)")
        print("="*80)
        print(f"{'æ¨¡å—å':<40} {'å¹³å‡è€—æ—¶ (ms)':<20} {'æ€»è€—æ—¶ (ms)':<20}")
        print("-"*80)

        # ç”¨äºèšåˆ alignment çš„æ€»æ—¶é—´
        alignment_times = []
        
        for name, times in sorted(self.timings.items()):
            if "alignment" in name:
                alignment_times.extend(times)

            avg_time = np.mean(times)
            total_time = np.sum(times)
            
            # åªæ‰“å°éèšåˆçš„ç»“æœ
            if "alignment_" in name:
                print(f"  - {name:<36} {avg_time:<20.3f} {total_time:<20.3f}")
            elif "alignment" not in name:
                 print(f"{name:<40} {avg_time:<20.3f} {total_time:<20.3f}")

        # è®¡ç®—å¹¶æ‰“å°èšåˆçš„ alignment æ—¶é—´
        if alignment_times:
            avg_alignment_time = np.mean(alignment_times) * len(self.modules_to_time['alignment'])
            total_alignment_time = np.sum(alignment_times)
            print("-"*80)
            print(f"{'alignment (èšåˆ)':<40} {avg_alignment_time:<20.3f} {total_alignment_time:<20.3f}")

        print("="*80)
# ============================================================================
# Part 3: è®­ç»ƒæ˜¾å­˜æµ‹è¯•ä¸»æ‰§è¡Œå—
# ============================================================================
def main():
    # --- 1. å‚æ•°é…ç½® ---
    parser = argparse.ArgumentParser(description="IMFModel Training Memory Test")
    parser.add_argument('--latent_dim', type=int, default=32, help='Dimension of the latent tokens.')
    # é˜ˆå€¼è®¾ä¸º 16*16=256ï¼Œè¿™æ · 8x8 å’Œ 16x16 å±‚ä¼šç”¨æ ‡å‡†æ³¨æ„åŠ›
    parser.add_argument('--swin_res_threshold', type=int, default=128, help='Resolution threshold to switch to Swin Attention.')
    parser.add_argument('--num_heads', type=int, default=8, help='Number of attention heads.')
    parser.add_argument('--window_size', type=int, default=8, help='Window size for Swin Attention.')
    parser.add_argument('--drop_path', type=float, default=0.1, help='Stochastic depth rate for Swin.')
    parser.add_argument('--low_res_depth', type=int, default=2, help='Number of TransformerBlocks for low-res features.')
    parser.add_argument('--batch_size', type=int, default=2, help='Batch size for training.')
    parser.add_argument('--img_size', type=int, default=256, help='Input image size.')
    
    args, _ = parser.parse_known_args()

    # --- 2. æ¨¡å‹åˆå§‹åŒ– (ä¿æŒä¸å˜) ---
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model = IMFModel(args).to(device)
    model.eval()

    # --- 3. å®šä¹‰è¦è®¡æ—¶çš„æ¨¡å— (ä¿æŒä¸å˜) ---
    modules_to_time = {
        'dense_feature_encoder': model.dense_feature_encoder,
        'latent_token_encoder': model.latent_token_encoder,
        'latent_token_decoder': model.latent_token_decoder,
        'alignment': model.implicit_motion_alignment, # è¿™æ˜¯ä¸€ä¸ª ModuleList
        'frame_decoder': model.frame_decoder,

    }

    # --- 4. åˆå§‹åŒ–è®¡æ—¶å™¨å¹¶æ³¨å†Œé’©å­ (ä¿æŒä¸å˜) ---
    print("\n" + "="*50)
    print("â±ï¸  æ­£åœ¨åˆå§‹åŒ–æ¨¡å—è®¡æ—¶å™¨å¹¶æ³¨å†Œé’©å­...")
    timer = ModuleTimer(modules_to_time, device)
    print("âœ… é’©å­æ³¨å†Œå®Œæˆ!")
    print("="*50)

    # --- 5. å‡†å¤‡è¾“å…¥æ•°æ® (ä¿æŒä¸å˜) ---
    batch_size = 1
    img_size = 256
    x_current = torch.randn(batch_size, 3, img_size, img_size).to(device)
    x_reference = torch.randn(batch_size, 3, img_size, img_size).to(device)
    
    # --- 6. è¿è¡Œé€Ÿåº¦æµ‹è¯• ---
    with torch.no_grad():
        # --- æ¨¡å‹é¢„çƒ­ ---
        print("\n" + "-"*50)
        print("æ­£åœ¨é¢„çƒ­æ¨¡å‹ (è¿è¡Œ10æ¬¡)...")
        for _ in range(10):
            _ = model(x_current, x_reference)
        print("é¢„çƒ­å®Œæˆã€‚")
        print("-"*50)

        # æ¸…é™¤é¢„çƒ­æœŸé—´çš„è®¡æ—¶æ•°æ®
        timer.reset()

        # --- æ€§èƒ½æµ‹è¯• ---
        num_runs = 1000
        print(f"\næ­£åœ¨ç²¾ç¡®æµ‹è¯• {num_runs} æ¬¡è¿è¡Œçš„æ¨¡å—å’Œæ€»é€Ÿåº¦...")
        
        # <--- æ–°å¢ï¼šä¸ºæ€»æ—¶é—´è®¡æ—¶åšå‡†å¤‡ ---
        if device.type == 'cuda':
            torch.cuda.synchronize()
        start_time_total = time.time()
        # <--- æ–°å¢ç»“æŸ ---

        for _ in range(num_runs):
            _ = model(x_current, x_reference)
            
        # <--- æ–°å¢ï¼šè®°å½•æ€»æ—¶é—´ç»“æŸå¹¶è®¡ç®— ---
        if device.type == 'cuda':
            torch.cuda.synchronize()
        end_time_total = time.time()
        
        total_elapsed_time = end_time_total - start_time_total
        avg_time_per_run_ms = (total_elapsed_time / num_runs) * 1000
        fps = num_runs / total_elapsed_time
        # <--- æ–°å¢ç»“æŸ ---
            
        print("âœ… é€Ÿåº¦æµ‹è¯•å®Œæˆ!")

    # --- 7. æ‰“å°æ¨¡å—è®¡æ—¶ç»“æœ ---
    timer.summary()

    # --- 8. æ–°å¢ï¼šæ‰“å°æ€»æ—¶é—´è®¡æ—¶ç»“æœ ---
    print("\n" + "="*80)
    print("ğŸš€ æ¨¡å‹æ€»ä½“æ€§èƒ½ (ç«¯åˆ°ç«¯)")
    print("="*80)
    print(f"{'æŒ‡æ ‡':<40} {'æ•°å€¼':<20}")
    print("-"*80)
    print(f"{'æ€»è€—æ—¶ (ç§’)':<40} {total_elapsed_time:<20.4f}")
    print(f"{'å¹³å‡æ¯æ¬¡å‰å‘ä¼ æ’­è€—æ—¶ (æ¯«ç§’)':<40} {avg_time_per_run_ms:<20.3f}")
    print(f"{'æ¨¡å‹ååç‡ (FPS)':<40} {fps:<20.2f}")
    print("="*80)

    # --- (å¯é€‰) æ‰“å°æ¨¡å‹æ€»å‚æ•° ---
    total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    print(f"\næ¨¡å‹æ€»å¯è®­ç»ƒå‚æ•°: {total_params / 1e6:.2f} M")


if __name__ == "__main__":
    main()